<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 4: Image Warping and Mosaicing</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0px;
            padding: 0;
        }
        header {
            background-color: #21b32d;
            color: white;
            padding: 10px 0;
            text-align: center;
        }
        .subheader {
            background-color: #f4f4f4;
            color: #333;
            padding: 10px 0;
            text-align: center;
        }
        .content {
            font-size: large;
            line-height: 1.5;
            padding: 20px;
            text-align: left;
            margin: 10px 150px 10px;
        }
        .p {
            margin: 10px 150px 10px;
        }
        .content img {
            max-width: 100%;
            height: auto;
        }
        .column {
          float: left;
          width: 33.1%;
          padding: 1px;
        }
        /* Clear floats after image containers */
        .row::after {
          content: "";
          clear: both;
          display: table;
        }
        .row {
            margin: 10px 150px 25px;
        }
        .column2 {
          float: left;
          width: 40%;
          padding: 1px;
        }
        /* Clear floats after image containers */
        .row2::after {
          content: "";
          clear: both;
          display: table;
        }
        .row2 {
            margin: 10px 150px 25px;
        }
        /* Centered caption*/
        figure {
          text-align: center; 
        }
        .container {
        display: flex;
        justify-content: space-around;
        align-items: flex-start;
        gap: 10px; /* Space between columns */
        }
        .image-column {
            flex: 1; /* Adjust the size of columns equally */
            text-align: center;
        }

        img {
            width: 80%;
            height: auto; /* Maintain aspect ratio */
        }

        figcaption {
            margin-top: 10px;
            font-weight: bold;
            text-align: center;
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<header>
    <h1>Project 4: Image Warping and Mosaicing</h1>
</header>

    
<div class="subheader">
    <h2>Overview</h2>
</div>
<div class="content">
    <p> The goal of this project is to make a panorama manually from multiple images captured from the same center of projection.
        This involves building an image mosaic by registering, projective warping, resampling, and compositing.
        Starting by shooting my own photographs, I first recover the homography between images and warped them accordingly
        onto a common projection plane. As a byproduct of this step, the homography matrix also enabled me to rectify any known 
        rectangular objects. Finally, the warped images were blended on the shared projection plane to produce the final mosaic.<br><br>
        Disclaimer: I downsized all the images to make the selection of correspondence points easier. This causes some of the images
        to be blurry, but it doesn't affect the warping and mosaicing process!
    </p>
</div>

    
<div class="subheader">
    <h2>Part 1: Shoot and Digitize Pictures</h2>
</div>
<div class="content">
    I shoot multiple photographs such that the transformation between them are projective. This is achieved by fixing the center
    of projection(camera) and rotate my body while capturing photos. In particular, I used zoomed out setting(x0.5) on the first example, hoping it will make
    the mosaic more interesting. Additionally, the photos are shoot close in time to avoid changes in lighting, and the fields of view
    are significantly overlapped to make the registration easier.
</div>

<div class="subheader">
    <h2>Part 2: Recover Homographies</h2>
</div>
<div class="content">
<p>
    Homography is a projective transformation such that from a single point (x, y) to (x', y'):
    \[
    \begin{bmatrix}
      a & b & c \\
      d & e & f \\
      g & h & 1
    \end{bmatrix}
    \begin{bmatrix}
      x \\
      y \\
      1
    \end{bmatrix}
    =
    \begin{bmatrix}
      wx' \\
      wy' \\
      w
    \end{bmatrix}
    \]
</p>
If we expand the matrix multiplication into systems of equation, this can be simplified to the following matrix operation:
<p>
    \[
    \begin{bmatrix}
      x & y & 1 & 0 & 0 & 0 & -xx' & -yx' \\
      0 & 0 & 0 & x & y & 1 & -xy' & -yy'
    \end{bmatrix}
    \begin{bmatrix}
      a \\
      b \\
      c \\
      d \\
      e \\
      f \\
      g \\
      h
    \end{bmatrix}
    =
    \begin{bmatrix}
      x' \\
      y'
    \end{bmatrix}
    \]
</p>

Given n correspondence points, it give us 2n many equations and 8 unknowns. This is an overconstrained system and can be solved
with least squares. I choose the naive solution <code>'np.linalg.lstsq()'</code> to find the parameters and the result is quite reasonable
(given the output of my warped images).
</div>

<div class="subheader">
    <h2>Part 3. Warp the Images</h2>
</div>
<div class="content">
    Given the homography matrix H, I used the inverse warping to avoid random holes in the warped image. First, I apply forward warping
    homography to the four corners to obtain the boundary of the warped image. With <code>'skimage.draw.polygon'</code>, it gives 
    me all the pixels that lies within the boundary. Then I apply inverse warping homography which maps all the points back to the 
    source image. To avoid potential aliasing in the resampling process, <code>'scipy.interpolate.griddata'</code> is used to 
    obtain the corresponding RGB channels. In between the processes, I apply translation to keep all points positive and make sure
    that the function runs smoothly
</div>
<div class="full">
    <figure>
        <img src="./media/3.1.png" style="width:90%">
    </figure> 
</div>


<div class="subheader">
    <h2>Part 4. Image Rectification</h2>
</div>

<div class="content">
    <p>
        In this section, I take a photo that contains a rubik's cube and a photo that contains a laptop. Both photos contain 
        a known rectangular objects, and my goal is to make them rectangular using a homography. The distinction compare 
        to the previous section is that I only have one image to work with this time, and the correspondence points can not 
        be draw so easily. The trick turns out to be pretty simple, I define the correspondences using what I know about the 
        rectangular object. For instance, if I want to rectify the rubik's cube, then I know a priori that it is a square, 
        I can arbitrarily define the correspondence to [0, 0], [0, 200], [200, 0], [200, 200] 
    </p>
</div>

<div class="full">
    <figure>
        <img src="./media/4.1.png" style="width:60%">
    </figure>
    <figure>
        <img src="./media/4.2.png" style="width:60%">
    </figure>
</div>


<div class="subheader">
    <h2>Part 5. Blend the images into a mosaic</h2>
</div>
<div class="content">
    With all the preparations complete, I can now align the images and blend them into a mosaic. The figure below showcases 
    one set of source images alongside their corresponding warped forms.
</div>
<div class="full">
    <figure>
        <img src="./media/5.1.png" style="width:100%">
    </figure> 
</div>


<div class="content">
    The first problem is alignment. Since the warped image is a translation of the source image, I manage to find the shifts
    in both x and y direction and pad them to my images appropriately. Next, I find the area of intersection and apply
    simple alpha average by setting alpha = 0.5. From the output mosaics, everything seems to be fine except on the edges of 
    intersection. This naive blending yields the result below. Note that the border artifact between im0 and im1 is much more 
    obstrude. This is because im1 and im2 are taking under a similar lighting setting, and im0 are taken facing the sunlight.
    I would imagine that my phone camera automatically adjust some settings such as exposure that cause those differences.

</div>
<div class="full">
    <figure>
        <img src="./media/5.2.png" style="width:50%">
        <img src="./media/5.3.png" style="width:70%">
    </figure> 
</div>


<div class="content">
    To mitigate edges, I applied a gradient mask that adjusts values based on the distance to one side of the overlapping region.
    With <code>'scipy.ndimage.distance_transform_edt'</code>, I was able to get the distance of each pixel to the border automatically.
    Then I normalized the intersection mask such that the highest value is set to one and use it as an alpha mask for my
    blending process. The result is quite amazing as shown below. I should also mention that the result is not perfect, likely due
    to numerical imprecision and integer rounding in my alignment stage. Also, the image is rather small in the middle because
    I took the photos with 0.5x zoom.
</div>

<div class="full">
    <figure>
        <img src="./media/5.4.png" style="width:70%">
    </figure> 
</div>

<div class="content">
    The figures below showcase two other examples of image mosaic :) <br>
    One attempt uses photos taken within a video game, ignoring user interface at the bottom, I think the screenshots blend pretty well!
</div>

<div class="full">
    <figure>
        <img src="./media/5.5.png" style="width:50%">
        <img src="./media/5.6.png" style="width:60%">
        <img src="./media/5.7.png" style="width:50%">
        <img src="./media/5.8.png" style="width:60%">
    </figure> 
</div>


<div class="subheader">
    <h2>Bells and Whistles: Fake Projection </h2>
</div>
<div class="content">
    I took a step further and attempt to project an among us figure onto the Berkeley tower!
    Procedurally, it is very similar to rectification but warped to a different perspective.
    The additional steps are the creation of a mask based on the white background and alignment to the tower.
    I performed a simple addition at the end, and the result gives the impression that the figure is being 
    projected onto the tower just like an actual projector would!
</div>

<div class="full">
    <figure>
        <img src="./media/b&w1.png" style="width:90%">
        <img src="./media/b&w2.png" style="width:50%">
    </figure> 
</div>


<header>
    <h2>Feature Matching and Autostitching</h2>
</header>
<div class="content">
    The work prior to this section relies heavily on the fact that the correspondence points are accurate, yet the determining of 
    correspondence points are largely by hand and can be time costly. Thus, we want to find a way to automatically label the
    correspondence, which are mostly corners! <br>
</div>

<div class="subheader">
    <h2>Step 1: Detecting Corner Features</h2>
</div>
<div class="content">
    We will start with harris interest point detector to automatically detect corners (inherently good correspondence choice).
    While the math and algorithm is not trivial, I utilize <Code>skimage.feature.corner_harris</Code> for naive implementation.
    Here is a generic algorithm in Harris detector behind the scene:
    <ol>
        <li>Compute Gaussian derivatives at each pixel
        <li>Compute second moment 2x2 matrix M in a Gaussian window around each pixel</li>
        <li>Compute corner response function f: \[f(x_i) = \frac{det(M)}{Trace(M)} = \frac{\lambda_1\lambda_2}{\lambda_1 + \lambda_2}\]</li>
        <li>Threshold f</li>
    </ol>
    However, the raw output returns more than 45000 corners, which would have cover my entire image if I were to plot it on a figure!
    I try two filters for visualization:
    <ol>
        <li>Percentile Filter: filters to corners with an h value above 99% percentile </li>
        <li>Max Pooling Filter: filters to corners that are local maximums of (50, 50) patch and has a h value above 0.25 (an empirical choice)</li>
    </ol>
    This is only for visualization because the next subsection in Adaptive Non-Maximal Suppression will sifted out most of them.
</div>
<div class="full">
    <figure>
        <img src="./media/step1.1.png" style="width:100%">
    </figure> 
</div>

<div class="subheader">
    <h2>Enhance Feature Selection: Adaptive Non-Maximal Suppression (ANMS)</h2>
</div>
<div class="content">
Now we have a way to detect features, we also want some good properties among the features! In particular, we want:
<ul>
    <li>Fixed number of features per image</li>
    <li>Spatially evenly distributed features</li>
</ul>
This motivates Adaptive Non-Maximal Suppression (ANMS), where I find the minimum suppression radius r<sub>i</sub> for each features.
Then 100 features with highest suppression radius are selected as the final subset. The idea of minimum suppression radius can be summarized 
in one phrase as "a distance to the nearest stronger keypoint." :)<br>
The mathematical definition for the minimum suppression radius is \[ r_i = \min_j \, \lvert \mathbf{x}_i - \mathbf{x}_j \rvert, \quad 
\text{s.t.} \quad f(\mathbf{x}_i) < c_{\text{robust}} f(\mathbf{x}_j) \]
where x<sub>i</sub>, x<sub>j</sub> are corner features, and f(x<sub>i</sub>), f(x<sub>j</sub>) are response values from harris corner.<br><br>
I picked the same c<sub>robust</sub> = 0.9 as in the paper to ensures that a neighbor must have significantly higher strength. <br>
In the figure below, left images use <Code>skimage.feature.peak_local_max</Code> with min_distance = 10, resulting in 3089 features.
The right images uses the output from <code> peak_local_max</code> as the input to ANMS, which further reduces to the top 100 features with desired properties.
</div>

<div class="full">
    <figure>
        <img src="./media/step1.2.png" style="width:100%">
    </figure> 
</div>

<div class="subheader">
    <h2>Step 2: Feature Descriptor</h2>
</div>
<div class="content">
    Given all the corner features, I need to somehow describe the feature with feature descriptors.
    For a basic implementation, an axis-aligned patch is used, assuming there are no rotational factor between images.
    <ol>
        <li>Blur the image to avoid aliasing in the feature descriptors</li>
        <li>For each corner feature, find the 40x40 window such that the corner is at the center</li>
        <li>Downsample by a factor of s = 5 such that the feature descriptor are 8x8</li>
        <li>Use bias/gain normalization and force the patch to have &mu; = 0 and &sigma; = 1</li>
    </ol>
    The figure below shows feature descriptors for corner features in moffitL.
</div>

<div class="full">
    <figure>
        <img src="./media/step2.1.png" style="width:40%">
    </figure> 
</div>


<div class="subheader">
    <h2>Step 3: Feature Matching</h2>
</div>
<div class="content">
    Lastly, I will need to find pairs of features that look similar.
    This is acheived by first flatten feature descriptors into vectors of length 8x8x3 = 192. Then I can compute the l<sub>2</sub> norm among all combinations
    between the two images to find the nearest neighbor. <br><br>
    Yet the issue is that if only
    one neighbor is used, two patches can be similar by chance or due to noise. Here, I apply Lowe's trick and compute the 
    <b>ratio between L2 of the closest match and the L2 of the second-closest match</b>.
    The idea is that if the ratio is small, then there is only one good match between the two features, which makes it more likely to be
    a true match!<br><br>
    In my work, I select a threshold of 0.4 to show that RANSAC does work in the next section.
    I observed that a threshold below 0.3 can already proceduce a flawless match in this set of images!<br><br>

    The figure below demonstrates the result after apply Lowe's trick. Points labels in blue are recognized as good matching features.
</div>

<div class="full">
    <figure>
        <img src="./media/step3.1.png" style="width:60%">
    </figure> 
</div>

<div class="subheader">
    <h2>Step 4: Random Sample Consensus(RANSAC)</h2>
</div>
<div class="content">
    With appropriate thresholding, Lowe's trick helps us to remove most of the outliers. However, the algorithm never guarentees that
    it will only return true matches. This is where RANSAC comes in, which automatically find the correct homography based on the
    set of good matches returned in feature matching section. It does so by filtering out the outliers and keep the largest set of
    inliers. The algorithm proceeds as follows:
    <ol>
        <li>For some large n
            <ol>
                <li>Select four feature pairs at random</li>
                <li>Compute the exact homography <b>H</b></li>
                <li>Compute <i>inliers</i> where <i>dist</i>(<b>x<sub>j</sub></b>, <b>Hx<sub>i</sub></b>) &lt; Îµ</li>
                <li>Keep largest set of inliers</li>
            </ol>
        </li>
        <li>Re-compute least-squares <b>H</b> estimate on all of the inliers</li>
    </ol>
</div>

<div class="full">
    <figure>
        <img src="./media/step4.1.png" style="width:60%">
        <img src="./media/step4.2.png" style="width:60%">
    </figure> 
</div>

<div class="subheader">
    <h2>Step 5: More Mosaics</h2>
</div>

<div class="content">
    <p>
    Below are the results using automatic feature matching and RANSAC. The alignment using the automated features
    are essentially indistinguishable from hand-picked correspondence (and I believe the features are even more accurate!). 
    Yet since least squared solution is used in computing the homography, the manual errors is mediated because 
    the algorithm can see them as random noise.
    </p>

    <p>
    I found the various method in auto-detecting corners to be the coolest thing. With different algorithms, we can keep refining
    down to the correct correspondences - even to features that can't be seen by eye! If we look at the visualization for 
    RANSAC correspondence careful, we can see that one of the features is detected on the smooth white wall!
    </p>

    <p>
    Moreover, the results here are potential better because I reimplement the distance blending 
    with <code>'scipy.ndimage.distance_transform_edt'</code>, and this time useing the full mask rather than just one edge.
    </p>
</div>

<div class="full">
    <figure>
        <img src="./media/step5.1.png" style="width:80%">
    </figure> 
</div>

<div class="container">
    <div class="image-column">
        <figure>
            <figcaption>Moffit with manual correspondences</figcaption>
            <img src="./media/5.8.png" alt="Image 1">
            
        </figure>
    </div>
    <div class="image-column">
        <figure>
            <figcaption>Moffit with automatic features and distance blending</figcaption>
            <img src="./media/step5.2.png" alt="Image 2">
        </figure>
    </div>
</div>

<div class="full">
    <figure>
        <img src="./media/step5.3.png" style="width:80%">
        <img src="./media/step5.4.png" style="width:80%">
    </figure> 
</div>

</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bridging Reality and Perception with Image Transformations in Anamorphic Art</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0px;
            padding: 0;
        }
        header {
            background-color: #21b32d;
            color: white;
            padding: 10px 0;
            text-align: center;
        }
        .subheader {
            background-color: #f4f4f4;
            color: #333;
            padding: 10px 0;
            text-align: center;
        }
        .content {
            font-size: large;
            line-height: 1.5;
            padding: 20px;
            text-align: left;
            margin: 10px 150px 10px;
        }
        .p {
            margin: 10px 150px 10px;
        }
        .content img {
            max-width: 100%;
            height: auto;
        }

        /* Centered caption*/
        figure {
          text-align: center; 
        }
        .container {
        display: flex;
        justify-content: space-around;
        align-items: flex-start;
        gap: 10px; /* Space between columns */
        }
        img {
            width: 80%;
            height: auto; /* Maintain aspect ratio */
        }
        .image-gallery {
            display: flex;
            justify-content: center;
            gap: 20px; 
        }
        .image-gallery img {
            width: 100%; 
            height: auto; 
            display: block;
        }

        .image-container {
            display: flex;
            flex-wrap: wrap;
            text-align: center;
            width: 100%;
            gap: 5px;
        }
        .image-container img {
            width: 100%;
            height: auto;
            border-radius: 8px;
        }
        .image-container figcaption {
            margin-top: 5px;
            font-style: italic;
        }
        figcaption {
            margin-top: 10px;
            font-weight: bold;
            text-align: center;
        }
        /*multi column image*/
        .column {
          float: left;
          width: 49.5%;
          padding: 1px;
        }
        /* Clear floats after image containers */
        .row::after {
          content: "";
          clear: both;
          display: table;
        }
        .row {
            margin: 10px 150px 25px;
        }
        video {
            border-radius: 8px;     /* Optional rounded corners */
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<header>
    <h1>Bridging Reality and Perception with Image Transformations in Anamorphic Art</h1>
</header>

    
<div class="subheader">
    <h2>Part I: Morphing Structure Out of Background</h2>
</div>
<div class="content">
    <p> 
        Before turning a 2D to a 3D, we aim to morph a pile of pixel into something that has shape, 
        and then into something that has both texture and shape. To do so, I interpolate color from the rest of the image
        onto the object and reverse the process to mimic the transformation from background to object.
    </p>
</div>

<div class="image-gallery">
    <figure class="imge-container" style="width: 300px; height: 450px;">
        <img src="./media/pre1.jpg">
        <figcaption>The Original Image</figcaption>
    </figure>
    <figure class="image-container" style="width: 300px; height: 450px;">
        <img src="./media/pre2.png">
        <figcaption>Inpaint From Around + Gaussian Blurred</figcaption>
    </figure> 
</div>

<div class="content">
    <p> 
        The first step is to erase the object form the image, this could easily be done using inpaint and Gaussian blur, 
        while there are still some artifacts (like the diagonal line due to inpainting), it is acceacptable as the object
        is no longer recognizable at first glance. However, simple reducing the inpaint or Gaussian blur intensity does not
        do the trick of reversing the cube back into object, there are two problems: the physical shape is decomposed under inpainting
        and illumination also looks weird, to address those two problems, I found a method called adaptive region sampling.
    </p>
</div>

<div class="image-gallery">
    <figure class="imge-container" style="width: 300px; height: 450px;">
        <img src="./media/pre3.png">
        <figcaption>Step 1: Manual Annotatation</figcaption>
    </figure>
    <figure class="image-container" style="width: 300px; height: 450px;">
        <img src="./media/pre4.png">
        <figcaption>Step 2: Finding the Visible Surface</figcaption>
    </figure> 
    <figure class="image-container" style="width: 300px; height: 450px;">
        <img src="./media/pre5.png">
        <figcaption>Step 3: Extend the Edge Between Surface</figcaption>
    </figure> 
    <figure class="image-container" style="width: 300px; height: 450px;">
        <img src="./media/pre6.png">
        <figcaption>Step 4: Use the Boundary Found to Segment Image Into Three Regions</figcaption>
    </figure> 
</div>

<div class="content">
    <p> 
        Step1: The process of this adaptive region sampling is as shown above, we first manually find the corner of the object,
        This process cannot be done using Harris Corner detection since the only corner
        we are looking for is the corner of our object, Harris would detect other corners in the image 
    </p>
    <p> 
        Step2: Using the manually annotated corners, we could get the visible surface, we would use this information
        to reconstruct the physical structure of the object
    </p>
    <p> 
        Step3: We extend the edge between the visible surfaces to make them touch the edge of the image.
        Those extended lines would help us segment the image
    </p>
    <p> 
        Step4: Using the boundary we found in Step3, we could segment our image into separate regions.
    </p>
</div>

<div class="image-gallery">
    <figure class="imge-container" style="width: 300px; height: 450px;">
        <img src="./media/pre7.png">
        <figcaption>The Region to Sample for 'Orange Surface' Shown in step 2</figcaption>
    </figure>
    <figure class="image-container" style="width: 300px; height: 450px;">
        <img src="./media/pre8.png">
        <figcaption>One Example Sample from the Aforementioned Region</figcaption>
    </figure> 
    <figure class="image-container" style="width: 300px; height: 450px;">
        <img src="./media/pre9.png">
        <figcaption>Interpolated 'Orange Surface' From Multiple Samples</figcaption>
    </figure> 
    <figure class="image-container" style="width: 300px; height: 450px;">
        <img src="./media/pre10.png">
        <figcaption>Fully Interpolated Object on a=0.3</figcaption>
    </figure> 
</div>


<div class="content">
    <p> 
        After finish the four steps above, we have the location of the visible surfaces and their corresponding selection regions.
        We could then sample a patch the same shape as the surface within the region to interpolate onto the target surface.
    </p>
    <p> 
        As shown in the example for 'orange surface' above, we do the sampling multiple times (I used 4-8 in my code),
        average the color for each pixel location, and blend it with the pixel of the target surface with a proportion alpha.
    </p>
    <p> 
        This method address the two issues we had before, since we are only sampling in a region that has similar illumination
        as our target surface, the result after blending is going to look nice illumination-wise. Since we reserved the physical
        structure of visible surface, as blending level become lower, the original object's physical strucure would become more and more clear.
    </p>
    <p> 
        Looking at the a=0.3 midway morphing image, we could see that the texture of the object is similar to the environment, 
        the upper surface looks like the tissue box and the left surface looks like the wood table.
        The illumination looks correct, the right surface appear to be darker than the left surface.
        The physical structure of the object is also identifiable, only a little vague.
        This shows that our sampling method works!
    </p>
</div>

<div class="full">
    <figure>
        <img src="./media/transform.png" style="width:70%">
    </figure> 
    <figure>
        <img src="./media/output.gif" style="width:10%">
        <figcaption>The Final Morphing Sequence</figcaption>
    </figure> 
</div>

<div class="content">
    <p> 
        Using the method above, we could adjust alpha level and produce a sequence of frames as above! 
    </p>
    <p> 
        With the 'morphed from background' object as the starting point, we could start to implement the rotation that
        rotates it into 3D!
    </p>
</div>
    
<div class="subheader">
    <h2>Preliminary: Shoot and Digitize Video</h2>
</div>
<div class="content">
    We recorded a short video of a Rubik's Cube with a moving camera. The shoot spans approximately 180 degrees, and the video was taken
    with flashlight turned on to minimize the shadow effect (although the result were not as effective as anticipated). Here
    are the video that were used as input in this project.
</div>

<div class="row">
    <div class="column">
        <figure>
            <video width="450" controls>
                <source src="./media/input_vid1.mp4" type="video/mp4">
            </video>
        </figure>
    </div>
    <div class="column">
        <figure>
            <video width="450" controls>
                <source src="./media/input_vid2.mp4" type="video/mp4">
            </video>
        </figure>
    </div>
</div>


<div class="subheader">
    <h2>Detecting Corner Features</h2>
</div>
<div class="content">
    We will start with harris interest point detector to automatically detect corners (inherently good correspondence choice).
    While the math and algorithm is not trivial, I utilize <Code>skimage.feature.corner_harris</Code> for naive implementation.
    Here is a generic algorithm in Harris detector behind the scene:
    <ol>
        <li>Compute Gaussian derivatives at each pixel
        <li>Compute second moment 2x2 matrix M in a Gaussian window around each pixel</li>
        <li>Compute corner response function f: \[f(x_i) = \frac{det(M)}{Trace(M)} = \frac{\lambda_1\lambda_2}{\lambda_1 + \lambda_2}\]</li>
        <li>Threshold f</li>
    </ol>
    However, the raw output returns more than 45000 corners, which would have cover my entire image if I were to plot it on a figure!
    Instead, I apply the Percentile Filter and filter to corners with an h value above 99% percentile.<br>
    This is only for visualization because the next subsection in Adaptive Non-Maximal Suppression will sifted out most of them.
</div>
<div class="full">
    <figure>
        <img src="./media/1.1.png" style="width:70%">
    </figure> 
</div>


<div class="subheader">
    <h2>Mask Segmentation with SAM </h2>
</div>
<div class="content">
    Segment Anything Model (SAM) is an advanced Segmentation Model developed by Meta. The <Code>AutomaticMaskGenerator</Code> 
    provides masks without labeling. Due to time constraint, we did not implement semantic segmentation for automatic labeling.
    Instead, bounding box and <Code>SamPredictor</Code> were used to obtain a mask for the object and a mask for the floor.

</div>
<div class="full">
    <figure>
        <img src="./media/mask.png" style="width:70%">
    </figure> 
</div>


<div class="subheader">
    <h2>Enhance Feature Selection: Adaptive Non-Maximal Suppression (ANMS)</h2>
</div>
<div class="content">
Now we have a way to detect features just on the floor surface, we also want some good properties among the features! In particular, we want:
<ul>
    <li>Fixed number of features per image</li>
    <li>Spatially evenly distributed features</li>
</ul>
This motivates Adaptive Non-Maximal Suppression (ANMS), where I find the minimum suppression radius r<sub>i</sub> for each features.
Then 100 features with highest suppression radius are selected as the final subset. The idea of minimum suppression radius can be summarized 
in one phrase as "a distance to the nearest stronger keypoint." :)<br>
The mathematical definition for the minimum suppression radius is \[ r_i = \min_j \, \lvert \mathbf{x}_i - \mathbf{x}_j \rvert, \quad 
\text{s.t.} \quad f(\mathbf{x}_i) < c_{\text{robust}} f(\mathbf{x}_j) \]
where x<sub>i</sub>, x<sub>j</sub> are corner features, and f(x<sub>i</sub>), f(x<sub>j</sub>) are response values from harris corner.<br><br>
I picked the same c<sub>robust</sub> = 0.9 as in the paper to ensures that a neighbor must have significantly higher strength. <br>
In the figure below, two arbitrary frames are choosen where I selected the top 100 features with desired properties.
</div>

<div class="full">
    <figure>
        <img src="./media/sample_ANMS.png" style="width:70%">
    </figure> 
</div>




<div class="subheader">
    <h2>Feature Descriptor</h2>
</div>
<div class="content">
    Given all the corner features, we need to somehow describe the feature with feature descriptors.
    With rotation and perspective distortion factors involved, we implement Scale Invariant Feature Transform (SIFT) descriptor.
    It utilizes gradient field around a feature:
    <ol>
        <li>Divide the bigger patch into 16 smaller image patches</li>
        <li>For each small image patch, generate a histogram of eight gradient orientations</li>
        <li>Stack the 16 patches and obtained a 8*16 = 128 feature descriptor vector (after flattening)</li>
    </ol>
    The figure below shows the mechanism behind SIFT.
</div>

<div class="full">
    <figure>
        <img src="./media/sift.jpg" style="width:40%">
    </figure> 
</div>


<div class="subheader">
    <h2>Feature Matching</h2>
</div>
<div class="content">
    Lastly, I will need to find pairs of features that look similar.
    This is acheived by first flatten feature descriptors into vectors of length 8x16 = 128. Then I can compute the l<sub>2</sub> norm among all combinations
    between the two images to find the nearest neighbor. <br><br>
    Yet the issue is that if only
    one neighbor is used, two patches can be similar by chance or due to noise. Here, I apply Lowe's trick and compute the 
    <b>ratio between L2 of the closest match and the L2 of the second-closest match</b>.
    The idea is that if the ratio is small, then there is only one good match between the two features, which makes it more likely to be
    a true match!<br><br>
    With some empirical testing, a Lowe's ratio of 0.55 is used in this project
    <br><br>

    The figure below demonstrates the result after apply Lowe's trick. Points labels in blue are recognized as good matching features.
</div>

<div class="full">
    <figure>
        <img src="./media/sample_LOWE.png" style="width:70%">
    </figure> 
</div>

<div class="subheader">
    <h2>Random Sample Consensus (RANSAC)</h2>
</div>
<div class="content">
    With appropriate thresholding, Lowe's trick helps us to remove most of the outliers. However, the algorithm never guarentees that
    it will only return true matches. This is where RANSAC comes in, which automatically find the correct homography based on the
    set of good matches returned in feature matching section. It does so by filtering out the outliers and keep the largest set of
    inliers. The algorithm proceeds as follows:
    <ol>
        <li>For some large n
            <ol>
                <li>Select four feature pairs at random</li>
                <li>Compute the exact homography <b>H</b></li>
                <li>Compute <i>inliers</i> where <i>dist</i>(<b>x<sub>j</sub></b>, <b>Hx<sub>i</sub></b>) &lt; Îµ</li>
                <li>Keep largest set of inliers</li>
            </ol>
        </li>
        <li>Re-compute least-squares <b>H</b> estimate on all of the inliers</li>
    </ol>
    In the first stage, the homography is found on consecutive frames. <br>
    Then we compute cumulative homographies using the composition property. 
    i.e. if H12 warp 1 to 2, H23 warp 2 to 3, then H13 = H23@H12 warp 1 to 3.
</div>

<div class="subheader">
    <h2>Warp to anamorphic illusion!</h2>
</div>

<div class="content">
    <p>
        In previous steps, we actually computed the inverse homography. This avoids the need of splatting in forward warping
        and <Code>Griddata</Code> is used to obtain the color value. Additionally, <Code>bwdist</Code> is used to implement
        distance-based blending. With the weight provided by bwdist, we can create alpha mask with the formula:
        \[
        a_1 = \frac{w_1}{w_1+w_2}, \quad a_2 = \frac{w_2}{w_1+w_2}
        \]
    </p>
</div>
<div class="full">
    <figure>
        <img src="./media/blending.png" style="width:50%">
    </figure> 
</div>

<div class="subheader">
    <h2>Project Demonstration</h2>
</div>
<div class="content">
    <p>
        The outputted video are in 30 fps, both has around 550~650 frames.
    </p>
</div>

<div class="row">
    <div class="column">
        <figure>
            <video width="450" controls>
                <source src="./media/output_vid1.av1.mp4" type="video/mp4">
            </video>
            <figcaption>Homography Composition: there are hundreds of compositions that leads to the ghosting effect.</figcaption>
        </figure>
    </div>
    <div class="column">
        <figure>
            <video width="450" controls>
                <source src="./media/output_vid2.av1.mp4" type="video/mp4">
            </video>
            <figcaption>Direct Homography to the reference frame: can't have a wider angle because the correspondences are no longer reliable.</figcaption>
        </figure>
    </div>
</div>

</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bridging Reality and Perception with Image Transformations in Anamorphic Art</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0px;
            padding: 0;
        }
        header {
            background-color: #21b32d;
            color: white;
            padding: 10px 0;
            text-align: center;
        }
        .subheader {
            background-color: #f4f4f4;
            color: #333;
            padding: 10px 0;
            text-align: center;
        }
        .content {
            font-size: large;
            line-height: 1.5;
            padding: 20px;
            text-align: left;
            margin: 10px 150px 10px;
        }
        .p {
            margin: 10px 150px 10px;
        }
        .content img {
            max-width: 100%;
            height: auto;
        }

        /* Centered caption*/
        figure {
          text-align: center; 
        }
        .container {
        display: flex;
        justify-content: space-around;
        align-items: flex-start;
        gap: 10px; /* Space between columns */
        }
        img {
            width: 80%;
            height: auto; /* Maintain aspect ratio */
        }

        figcaption {
            margin-top: 10px;
            font-weight: bold;
            text-align: center;
        }
        /*multi column image*/
        .column {
          float: left;
          width: 49.5%;
          padding: 1px;
        }
        /* Clear floats after image containers */
        .row::after {
          content: "";
          clear: both;
          display: table;
        }
        .row {
            margin: 10px 150px 25px;
        }
        video {
            border-radius: 8px;     /* Optional rounded corners */
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<header>
    <h1>Bridging Reality and Perception with Image Transformations in Anamorphic Art</h1>
</header>

    
<div class="subheader">
    <h2>Interpolating color from the surronding pixels</h2>
</div>
<div class="content">
    <p> 
        some text
    </p>
</div>

<div class="full">
    <figure>
        <img src="./media/pre1.png" style="width:70%">
        <img src="./media/pre2.jpg" style="width:70%">
        <img src="./media/pre3.gif" style="width:30%">
    </figure> 
</div>

    
<div class="subheader">
    <h2>Preliminary: Shoot and Digitize Video</h2>
</div>
<div class="content">
    We recorded a short video of a Rubik's Cube with a moving camera. The shoot spans approximately 180 degrees, and the video was taken
    with flashlight turned on to minimize the shadow effect (although the result were not as effective as anticipated). Here
    is the video that is used as input in this project.
</div>
<figure>
    <video width="450" controls>
        <source src="./media/vid_evans.mp4" type="video/mp4">
    </video>
</figure>



<div class="subheader">
    <h2>Step 1: Detecting Corner Features</h2>
</div>
<div class="content">
    We will start with harris interest point detector to automatically detect corners (inherently good correspondence choice).
    While the math and algorithm is not trivial, I utilize <Code>skimage.feature.corner_harris</Code> for naive implementation.
    Here is a generic algorithm in Harris detector behind the scene:
    <ol>
        <li>Compute Gaussian derivatives at each pixel
        <li>Compute second moment 2x2 matrix M in a Gaussian window around each pixel</li>
        <li>Compute corner response function f: \[f(x_i) = \frac{det(M)}{Trace(M)} = \frac{\lambda_1\lambda_2}{\lambda_1 + \lambda_2}\]</li>
        <li>Threshold f</li>
    </ol>
    However, the raw output returns more than 45000 corners, which would have cover my entire image if I were to plot it on a figure!
    Instead, I apply the Percentile Filter and filter to corners with an h value above 99% percentile.<br>
    This is only for visualization because the next subsection in Adaptive Non-Maximal Suppression will sifted out most of them.
</div>
<div class="full">
    <figure>
        <img src="./media/1.1.png" style="width:70%">
    </figure> 
</div>



<div class="subheader">
    <h2>Enhance Feature Selection: Adaptive Non-Maximal Suppression (ANMS)</h2>
</div>
<div class="content">
Now we have a way to detect features, we also want some good properties among the features! In particular, we want:
<ul>
    <li>Fixed number of features per image</li>
    <li>Spatially evenly distributed features</li>
</ul>
This motivates Adaptive Non-Maximal Suppression (ANMS), where I find the minimum suppression radius r<sub>i</sub> for each features.
Then 100 features with highest suppression radius are selected as the final subset. The idea of minimum suppression radius can be summarized 
in one phrase as "a distance to the nearest stronger keypoint." :)<br>
The mathematical definition for the minimum suppression radius is \[ r_i = \min_j \, \lvert \mathbf{x}_i - \mathbf{x}_j \rvert, \quad 
\text{s.t.} \quad f(\mathbf{x}_i) < c_{\text{robust}} f(\mathbf{x}_j) \]
where x<sub>i</sub>, x<sub>j</sub> are corner features, and f(x<sub>i</sub>), f(x<sub>j</sub>) are response values from harris corner.<br><br>
I picked the same c<sub>robust</sub> = 0.9 as in the paper to ensures that a neighbor must have significantly higher strength. <br>
In the figure below, left images use <Code>skimage.feature.peak_local_max</Code> with min_distance = 10, resulting in 3089 features.
The right images uses the output from <code> peak_local_max</code> as the input to ANMS, which further reduces to the top 100 features with desired properties.
</div>

<div class="full">
    <figure>
        <img src="./media/step1.2.png" style="width:100%">
    </figure> 
</div>


<div class="subheader">
    <h2>Segment Anything Model</h2>
</div>
<div class="content">
    text
</div>



<div class="subheader">
    <h2>Step 2: Feature Descriptor</h2>
</div>
<div class="content">
    Given all the corner features, I need to somehow describe the feature with feature descriptors.
    For a basic implementation, an axis-aligned patch is used, assuming there are no rotational factor between images.
    <ol>
        <li>Blur the image to avoid aliasing in the feature descriptors</li>
        <li>For each corner feature, find the 40x40 window such that the corner is at the center</li>
        <li>Downsample by a factor of s = 5 such that the feature descriptor are 8x8</li>
        <li>Use bias/gain normalization and force the patch to have &mu; = 0 and &sigma; = 1</li>
    </ol>
    The figure below shows feature descriptors for corner features in moffitL.
</div>

<div class="full">
    <figure>
        <img src="./media/step2.1.png" style="width:40%">
    </figure> 
</div>


<div class="subheader">
    <h2>Step 3: Feature Matching</h2>
</div>
<div class="content">
    Lastly, I will need to find pairs of features that look similar.
    This is acheived by first flatten feature descriptors into vectors of length 8x8x3 = 192. Then I can compute the l<sub>2</sub> norm among all combinations
    between the two images to find the nearest neighbor. <br><br>
    Yet the issue is that if only
    one neighbor is used, two patches can be similar by chance or due to noise. Here, I apply Lowe's trick and compute the 
    <b>ratio between L2 of the closest match and the L2 of the second-closest match</b>.
    The idea is that if the ratio is small, then there is only one good match between the two features, which makes it more likely to be
    a true match!<br><br>
    In my work, I select a threshold of 0.4 to show that RANSAC does work in the next section.
    I observed that a threshold below 0.3 can already proceduce a flawless match in this set of images!<br><br>

    The figure below demonstrates the result after apply Lowe's trick. Points labels in blue are recognized as good matching features.
</div>

<div class="full">
    <figure>
        <img src="./media/step3.1.png" style="width:60%">
    </figure> 
</div>

<div class="subheader">
    <h2>Step 4: Random Sample Consensus(RANSAC)</h2>
</div>
<div class="content">
    With appropriate thresholding, Lowe's trick helps us to remove most of the outliers. However, the algorithm never guarentees that
    it will only return true matches. This is where RANSAC comes in, which automatically find the correct homography based on the
    set of good matches returned in feature matching section. It does so by filtering out the outliers and keep the largest set of
    inliers. The algorithm proceeds as follows:
    <ol>
        <li>For some large n
            <ol>
                <li>Select four feature pairs at random</li>
                <li>Compute the exact homography <b>H</b></li>
                <li>Compute <i>inliers</i> where <i>dist</i>(<b>x<sub>j</sub></b>, <b>Hx<sub>i</sub></b>) &lt; Îµ</li>
                <li>Keep largest set of inliers</li>
            </ol>
        </li>
        <li>Re-compute least-squares <b>H</b> estimate on all of the inliers</li>
    </ol>
</div>

<div class="full">
    <figure>
        <img src="./media/step4.1.png" style="width:60%">
        <img src="./media/step4.2.png" style="width:60%">
    </figure> 
</div>

<div class="subheader">
    <h2>Step 5: More Mosaics</h2>
</div>

<div class="content">
    <p>
    Below are the results using automatic feature matching and RANSAC. The alignment using the automated features
    are essentially indistinguishable from hand-picked correspondence (and I believe the features are even more accurate!). 
    Yet since least squared solution is used in computing the homography, the manual errors is mediated because 
    the algorithm can see them as random noise.
    </p>

    <p>
    I found the various method in auto-detecting corners to be the coolest thing. With different algorithms, we can keep refining
    down to the correct correspondences - even to features that can't be seen by eye! If we look at the visualization for 
    RANSAC correspondence careful, we can see that one of the features is detected on the smooth white wall!
    </p>

    <p>
    Moreover, the results here are potential better because I reimplement the distance blending 
    with <code>'scipy.ndimage.distance_transform_edt'</code>, and this time useing the full mask rather than just one edge.
    </p>
</div>

<div class="full">
    <figure>
        <img src="./media/step5.1.png" style="width:80%">
    </figure> 
</div>

<div class="container">
    <div class="image-column">
        <figure>
            <figcaption>Moffit with manual correspondences</figcaption>
            <img src="./media/5.8.png" alt="Image 1">
            
        </figure>
    </div>
    <div class="image-column">
        <figure>
            <figcaption>Moffit with automatic features and distance blending</figcaption>
            <img src="./media/step5.2.png" alt="Image 2">
        </figure>
    </div>
</div>

<div class="full">
    <figure>
        <img src="./media/step5.3.png" style="width:80%">
        <img src="./media/step5.4.png" style="width:80%">
    </figure> 
</div>

</body>
</html>
